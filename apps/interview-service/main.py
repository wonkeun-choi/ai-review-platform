import os
import google.generativeai as genai
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List

app = FastAPI()

# ----------------------------------------------------
# CORS ì„¤ì • (web/vite:3000 í—ˆìš©)
# ----------------------------------------------------
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ----------------------------------------------------
# Gemini AI ì„¤ì •
# ----------------------------------------------------
try:
    genai.configure(api_key=os.environ.get("GEMINI_API_KEY"))
    model = genai.GenerativeModel('gemini-1.5-flash')
except Exception as e:
    print(f"Error configuring Gemini: {e}")
    model = None

# ----------------------------------------------------
# ğŸš€ (ì‹ ê·œ) AI ë©´ì ‘ê´€ ì—­í•  ë¶€ì—¬ (System Prompt)
# ----------------------------------------------------
INTERVIEWER_PROMPT = """
You are a senior technical interviewer at a major tech company. 
Your role is to conduct a technical interview based on the user's chosen topic.

Your task:
1.  Start by asking a foundational question about the chosen topic.
2.  Receive the user's answer.
3.  Provide brief, constructive feedback on their answer (e.g., "That's correct," "That's partially right, but you missed...", "Could you elaborate on...").
4.  After giving feedback, ask ONE clear follow-up question that builds upon their answer or explores a related concept.
5.  Maintain a professional, encouraging, yet challenging tone.
6.  Do NOT provide the full correct answer yourself, but guide the user toward it.
"""

# ----------------------------------------------------
# API Pydantic ëª¨ë¸ ì •ì˜
# ----------------------------------------------------
class ChatMessage(BaseModel):
    role: str  # "user" or "model"
    text: str

class ChatRequest(BaseModel):
    topic: str               # e.g., "React", "Python", "Data Structures"
    history: List[ChatMessage] # ì´ì „ ëŒ€í™” ë‚´ìš©
    user_message: str        # ì‚¬ìš©ìì˜ í˜„ì¬ ë‹µë³€

# ----------------------------------------------------
# API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
# ----------------------------------------------------
@app.post("/api/interview/chat")
async def handle_interview_chat(request: ChatRequest):
    if not model:
        return {"error": "Gemini AI model is not configured."}

    try:
        # 1. ëŒ€í™” ë§¥ë½(History) êµ¬ì„±
        #    [ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸] + [ì´ì „ ëŒ€í™”] + [í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€]
        formatted_history = [
            {"role": "user", "parts": [INTERVIEWER_PROMPT + f"\nThe chosen topic is: {request.topic}"]} ,
            {"role": "model", "parts": ["Understood. I will act as a senior technical interviewer. Let's begin."]}
        ]
        
        for msg in request.history:
            formatted_history.append({"role": msg.role, "parts": [msg.text]})
        
        # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        formatted_history.append({"role": "user", "parts": [request.user_message]})
        
        # 2. Gemini API í˜¸ì¶œ
        chat_session = model.start_chat(history=formatted_history[:-1]) # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì œì™¸í•˜ê³  íˆìŠ¤í† ë¦¬ë¡œ ì‹œì‘
        response = chat_session.send_message(request.user_message) # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì „ì†¡
        
        # 3. AIì˜ ì‘ë‹µ (ë‹¤ìŒ ì§ˆë¬¸ ë˜ëŠ” í”¼ë“œë°±) ë°˜í™˜
        return {"response": response.text}

    except Exception as e:
        return {"error": f"Failed to get chat response: {str(e)}"}

@app.get("/")
def read_root():
    return {"status": "Interview Service is running."}